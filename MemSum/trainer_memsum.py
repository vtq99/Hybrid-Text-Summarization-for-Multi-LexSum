# -*- coding: utf-8 -*-
"""Trainer - MemSum.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J8Hs2Aj9TFCgrKu4MooPDwNtWWqPhu9S

# Preparation

## Clone the Repo
"""

from google.colab import drive
drive.mount('/content/drive')

file_range = (0, 100)

TRAIN = '/content/drive/MyDrive/Text Summarization/MemSum/data/golden_16k/train_labelled_1000-1500-Version2.jsonl'
VALIDATION = '/content/drive/MyDrive/Text Summarization/MemSum/data/golden_16k/validation_labelled_0-200.jsonl'

!git clone https://github.com/nianlonggu/MemSum.git

"""## Change the working directory to the main folder of MemSum"""

import os
os.chdir("/content/drive/MyDrive/Text Summarization/MemSum")

"""## Install packages

Note: Because colab has preinstalled torch, so we don't need to install pytorch again

We tested on torch version>=1.11.0.
"""

!pip install -r requirements.txt -q

import torch
torch.__version__

"""# Preprocessing Custom data

Suppose that you have already splitted the training / validation and  test set:

The training data is now stored in a .jsonl file that contains a list of json info, one line for one training instance. Each json (or dictonary) contains two keys: 

1. "text": the value for which is a python list of sentences, this represents the document you want to summarize;
2. "summary": the value is also a list of sentences. If represent the ground-truth summary. Because the summary can contain multiple sentences, so we store them as a list.

The same for the validation file and the testing file.
"""

import json
train_corpus = [ json.loads(line) for line in open("data/golden_16k/train_golden.jsonl") ]

train_corpus = train_corpus[file_range[0]:file_range[1]]
## as an example, we have 100 instances for training
print(len(train_corpus))
print(train_corpus[0].keys())
print(train_corpus[0]["text"][:3])
print(train_corpus[0]["summary"][:3])

"""If you have your own data, process them into the same structure then put them into the data/ folder

The next thing we need to do is to create high-ROUGE episodes for the training set, as introduced in the paper: https://aclanthology.org/2022.acl-long.450/,
and the github introduction: https://github.com/nianlonggu/MemSum#addition-info-code-for-obtaining-the-greedy-summary-of-a-document-and-creating-high-rouge-episodes-for-training-the-model
"""

from src.data_preprocessing.MemSum.utils import greedy_extract
import json
from tqdm import tqdm

# train_corpus = [ json.loads(line) for line in open("data/formatted_16k/train_CUSTOM_raw.jsonl") ]
train_corpus
for data in tqdm(train_corpus):
    high_rouge_episodes = greedy_extract( data["text"], data["summary"], beamsearch_size = 2 )
    indices_list = []
    score_list  = []

    for indices, score in high_rouge_episodes:
        indices_list.append( indices )
        score_list.append(score)

    data["indices"] = indices_list
    data["score"] = score_list

"""Now we have obtained the labels for the training set. This can be parallized if you have large training set.

We can save the labeled training set to a new file:
"""

with open(f"data/formatted_16k/train_labelled_{file_range[0]}-{file_range[1]}.jsonl","w") as f:
    for data in train_corpus:
        f.write(json.dumps(data) + "\n")

"""That's it! We are about to train MemSum!"""



"""# Training

## Download pretrained word embedding

MemSUM used the glove embedding (200dim), with three addition token embeddings for bos eos pad, etc.

You can download the word embedding (a folder named glove/) used in this work:

https://drive.google.com/drive/folders/1lrwYrrM3h0-9fwWCOmpRkydvmF6hmvmW?usp=sharing

and put the folder under the model/ folder. 

Or you can do it using the code below:

Make sure the structure looks like:

1. MemSum/model/glove/unigram_embeddings_200dim.pkl
2. MemSum/model/glove/vocabulary_200dim.pkl


If not, you can change manually
"""

!pip install gdown -q
try:
    os.system("rm -r model")
    os.makedirs("model/")
except:
    pass
!cd model/; gdown --folder "https://drive.google.com/drive/folders/1lrwYrrM3h0-9fwWCOmpRkydvmF6hmvmW"


if not os.path.exists("model/glove"):
    try:
        os.makedirs("model/glove")
        os.system("mv model/*.pkl model/glove/")
    except:
        pass

"""## Start training

Note:
1. you need to switch to the folder src/MemSum_Full;
2. You can specify the path to training and validation set, the model_folder (where you want to store model checkpoints) and the log_folder (where you want to store the log info), and other parameters. 
3. You can provide the absolute path, or relative path, as shown in the example code below.
4. n_device means the number of available GPUs
"""



os.path.abspath(os.getcwd())

"""../content/drive/MyDrive/Text Summarization/glove
/content/drive/MyDrive/Text Summarization/glove/unigram_embeddings_200dim.pkl

---


"""

!cd src/MemSum_Full; python train.py -training_corpus_file_name "/content/drive/MyDrive/Text Summarization/MemSum/data/golden_16k/train_full_labelled.jsonl" -validation_corpus_file_name "/content/drive/MyDrive/Text Summarization/MemSum/data/golden_16k/validation_golden.jsonl"  -model_folder ../../model/MemSum_Full/golden2/200dim/run0/ -log_folder ../../log/MemSum_Full/golden2/200dim/run0/ -vocabulary_file_name ../../model/glove/vocabulary_200dim.pkl -pretrained_unigram_embeddings_file_name ../../model/glove/unigram_embeddings_200dim.pkl -max_seq_len 100 -max_doc_len 500 -num_of_epochs 10 -save_every 1000 -n_device 1 -batch_size_per_device 4 -max_extracted_sentences_per_document 400 -moving_average_decay 0.999 -p_stop_thres 0.6

"""# Testing trained model on custom dataset"""

from summarizers import MemSum
from tqdm import tqdm
from rouge_score import rouge_scorer
import json
import numpy as np

rouge_cal = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeLsum'], use_stemmer=True)

memsum_custom_data = MemSum(  "/MemSum/model_batch_8672.pt", 
                  "/MemSum/glove/vocabulary_200dim.pkl", 
                  gpu = 0 ,  max_doc_len = 500  )

test_corpus_custom_data = [ json.loads(line) for line in open("/content/drive/MyDrive/Text Summarization/MemSum/data/golden_16k/test_golden.jsonl")]

def evaluate( model, corpus, p_stop, max_extracted_sentences, rouge_cal ):
    f = open("/content/drive/MyDrive/Text Summarization/MemSum/data/golden_16k/test_results_100.jsonl", "w")
    
    scores = []
    for data in tqdm(corpus):
        gold_summary = data["summary"]
        extracted_summary = model.extract( [data["text"]], p_stop_thres = p_stop, max_extracted_sentences_per_document = max_extracted_sentences )[0]
        
        result = dict(target=gold_summary, extracted=extracted_summary)
        f.write(json.dumps(result) + '\n')

        score = rouge_cal.score( "\n".join( gold_summary ), "\n".join(extracted_summary)  )
        scores.append( [score["rouge1"].fmeasure, score["rouge2"].fmeasure, score["rougeLsum"].fmeasure ] )

    f.close()
    return np.asarray(scores).mean(axis = 0)

evaluate( memsum_custom_data, test_corpus_custom_data, 0.6, 400, rouge_cal )



"""To cite MemSum, please use the following bibtex:

```
@inproceedings{gu-etal-2022-memsum,
    title = "{M}em{S}um: Extractive Summarization of Long Documents Using Multi-Step Episodic {M}arkov Decision Processes",
    author = "Gu, Nianlong  and
      Ash, Elliott  and
      Hahnloser, Richard",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.450",
    doi = "10.18653/v1/2022.acl-long.450",
    pages = "6507--6522",
    abstract = "We introduce MemSum (Multi-step Episodic Markov decision process extractive SUMmarizer), a reinforcement-learning-based extractive summarizer enriched at each step with information on the current extraction history. When MemSum iteratively selects sentences into the summary, it considers a broad information set that would intuitively also be used by humans in this task: 1) the text content of the sentence, 2) the global text context of the rest of the document, and 3) the extraction history consisting of the set of sentences that have already been extracted. With a lightweight architecture, MemSum obtains state-of-the-art test-set performance (ROUGE) in summarizing long documents taken from PubMed, arXiv, and GovReport. Ablation studies demonstrate the importance of local, global, and history information. A human evaluation confirms the high quality and low redundancy of the generated summaries, stemming from MemSum{'}s awareness of extraction history.",
}
```
"""

