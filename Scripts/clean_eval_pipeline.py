# -*- coding: utf-8 -*-
"""CLEAN_eval_pipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LT0zCL_QAmhc9TJrh8tycY8AbiN_-CKk
"""

import nltk
nltk.download('punkt')
import torch
import transformers
from transformers import LEDTokenizer, LEDForConditionalGeneration
from transformers import AutoTokenizer
from UniEval.utils import convert_to_json
from UniEval.metric.evaluator import get_evaluator
from MemSum.summarizers import MemSum
from tqdm import tqdm
from rouge_score import rouge_scorer
import json
import numpy as np

#You need upload utils.py and metrics folder from https://github.com/maszhongming/UniEval
#and summarizers.py https://github.com/nianlonggu/MemSum to the server

rouge_cal = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeLsum'], use_stemmer=True)
evaluator = get_evaluator('summarization')

#Change these 3 file directories
extractive_model = MemSum(  "model.pt", 
                  "vocabulary.pkl", 
                  gpu = 0 ,  max_doc_len = 500  )
test_corpus_custom_data = [ json.loads(line) for line in open("test_golden.jsonl")]

#i can only see normal on your hugging face BECAUSE IT WAS PRIVATE SORRY I MADE IT PUBLIC NOW
model = LEDForConditionalGeneration.from_pretrained("hebaabdelrazek/led-multilexsum-normal").to("cuda")
tokenizer = LEDTokenizer.from_pretrained("allenai/led-base-16384-multi_lexsum-source-long", truncation=True, truncation_side='right', model_max_length=16384)

src_list, ref_list, output_list = [], [], []
scores = []

f = open("test_results_full.jsonl", "w")
for data in tqdm(test_corpus_custom_data):
    extracted_summary = extractive_model.extract( [data["text"]], p_stop_thres = 0.6, max_extracted_sentences_per_document = 400 )[0]
    proxy = " ".join(extracted_summary)
    inputs = tokenizer.encode(proxy, return_tensors="pt", truncation=True, max_length=16384).to("cuda")
    summary = model.generate(inputs, num_beams=5, max_length=1024, no_repeat_ngram_size = 3)
    summary_decoded = tokenizer.decode(summary[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)   
    src_list.append(" ".join(data["text"]))
    ref_list.append(" ".join(data["summary"]))
    output_list.append(summary_decoded)

    result = dict(input=data["text"], proxy=proxy, output=summary_decoded, target=" ".join(data["summary"]))
    f.write(json.dumps(result) + '\n')

    score = rouge_cal.score(" ".join(data["summary"]), summary_decoded)
    scores.append( [score["rouge1"].fmeasure, score["rouge2"].fmeasure, score["rougeLsum"].fmeasure ] )

f.close()

rouges = np.asarray(scores).mean(axis = 0)
print(rouges)
with open('scores.npy', 'wb') as fs:
    np.save(fs, scores)

data = convert_to_json(output_list=output_list, 
                       src_list=src_list, ref_list=ref_list)
eval_scores = evaluator.evaluate(data, print_result=True)
with open('eval_scores.json', 'w') as fe:
    fe.write(json.dumps(eval_scores))

