# -*- coding: utf-8 -*-
"""Budget reformatlabels.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1prhUqXgR8DXur4fIXHaP0EBiq25VQQWh
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install datasets==1.2.1
# !pip install transformers==4.2.0
# !pip install rouge_score
# !pip install tqdm
from datasets import load_dataset, load_metric
from rouge_score import rouge_scorer
from transformers import LEDTokenizer
import json
import random

# data_type = "primera" 
data_type = "led"

if(data_type=="primera"):
  print("Using 4000 budget and Primera tokenizer")
  tokenizer = LEDTokenizer.from_pretrained("allenai/PRIMERA")
  BUDGET = 4000

elif(data_type=="led"):
  print("Using 16000 budget and LED tokenizer")
  tokenizer = LEDTokenizer.from_pretrained("allenai/led-base-16384")
  BUDGET = 16000

def rouge2_scorer(sentence, target):
  return rouge_scorer.RougeScorer(['rouge2'], use_stemmer=True).score(sentence, target)['rouge2'].recall

source = open("data_full/test_final.jsonl", "r")

lines = source.readlines()
source.close()

clean_dataset = open("test_full_formattated_16K_noise.jsonl", "w")

def add_unimportant(selected_dct):
  random.seed(5)
  # print(selected_dct)
  selected_dct2 = selected_dct.copy()
  for key in selected_dct2.keys():
    
    value = selected_dct2[key]
    if len(value)==0:
      continue
    # print(value)
    max_sentence_index = sorted(value)[-1]
    min_sentence_index = sorted(value)[0]

    unimportant_potential = list(range(min_sentence_index, max_sentence_index+1))
    unimportant = sorted((list(set(unimportant_potential) - set(selected_dct2[key]))))
    number = int(random.choice(range(5, 20))/100*len(selected_dct2[key]))
    
    # We need to remove equal number of sentences from the selected as much as we are adding
    if(len(unimportant)>0):
      candidates = random.choices(unimportant, k=number)
      random.shuffle(value)
      selected_dct2[key] = sorted(value[:len(value)-len(candidates)])
      selected_dct2[key] = (list(set(selected_dct2[key] + candidates)))
 
  return selected_dct2

line_counter=0
for line in lines:
    print("Sample ", line_counter)
    line_counter+=1
    json_line = json.loads(line)

    new_format = dict()
    source = json_line.get("source")
    new_format["target"] = json_line.get("summary")

    extracted_lines = []
    # The reason why we add this noise in selected_dict as well is because if all sentences were 
    # selected in the score, the noise method below will not add noise, so from this alteration 
    # we can guarantee we have added some noise level in all samples. Even those with too little tokens.
    noisy_labels = add_unimportant(json_line["label"])
    for key in noisy_labels.keys():
         extracted_lines += [source[str(key)][slt] for slt in noisy_labels[key]]
    
    scores = []
    idx=0
    for extracted_line in extracted_lines:
      scores.append((idx, extracted_line, rouge2_scorer(extracted_line, json_line.get("summary"))))
      idx+=1
    
    scores_sorted = sorted(scores, key=lambda x: x[2], reverse=True)
   
    ordered_budget = []
    budget_extracted_lines = ''
    count = 0
    idx_array_selected =[]
    for idx, sentence, _ in scores_sorted:
      if len(tokenizer(budget_extracted_lines + sentence).get("input_ids")) < BUDGET:
        budget_extracted_lines += sentence
        budget_extracted_lines += " "
        count += 1

        idx_array_selected.append(idx)
        ordered_budget.append((idx,sentence))
      else:
        break
    
    ################################# Noise injection code starts here ##############################
    
    number_of_sentences_selected = len(ordered_budget)

    # We take from 5-20% relative to number of selected sentences as the unimportant sentences random choice
    number_of_sentences_umimportant = int(random.choice(range(5,20))/100*number_of_sentences_selected)
    
    min_idx_selected = min(ordered_budget, key=lambda x: x[0])[0]
    max_idx_selected = max(ordered_budget, key=lambda x: x[0])[0]

    # Generate potential idx that can be unimportant
    unimportant_potential_sentences = list(range(min_idx_selected, max_idx_selected+1))

    # Generate the unimportant indexes we can choose from
    unimportant = sorted((list(set(unimportant_potential_sentences) - set(idx_array_selected))))

    # If we can add noise, remove equal number of sentences and substritute
    if(len(unimportant)>0):
      
      if(len(unimportant)<number_of_sentences_umimportant):
        number_of_sentences_umimportant = len(unimportant)
      
      unimportant_candidates = unimportant[:number_of_sentences_umimportant]
     
      random.shuffle(idx_array_selected)

      idx_array_selected = idx_array_selected[:len(idx_array_selected)-len(unimportant_candidates)]
      idx_array_selected = sorted(list(set(idx_array_selected + unimportant_candidates)))

      # Now we have to edit the ordered_budget to include the noise
      noisy_ordered_budget = []
      for idx, sentence, _ in scores:
        if idx in idx_array_selected:
          noisy_ordered_budget.append((idx, sentence))
      print("Noise added")
      ordered_budget = noisy_ordered_budget
    ################################# Noise injection code ends here ##############################

    ordered_budget_final = sorted(ordered_budget, key=lambda x: x[0], reverse=False)
    final_sentences = ''
    for _ , sentence in ordered_budget_final:
      final_sentences += sentence
      final_sentences += " "

    print("Before ",len(scores_sorted),"After ", count)

    new_format["source"] = final_sentences
    print("Length", len(tokenizer(budget_extracted_lines).get("input_ids")))
    clean_dataset.write(json.dumps(new_format) + "\n" )

clean_dataset.close()
